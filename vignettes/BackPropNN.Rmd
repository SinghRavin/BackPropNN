---
title: "BackPropNN"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{BackPropNN}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(BackPropNN)
```

# Simulated data - Logistics data.

```{r}
set.seed(100)
num_obs <- 10000 # Number of observations

# Setting coefficients values for the logit function.
beta0 <- -2.5
beta1 <- 0.02
beta2 <- 0.01

# Simulating the independent variables.
X1 <- runif(n=num_obs, min=18, max=60)
X2 <- runif(n=num_obs, min=100, max=250)
prob <- exp(beta0 + beta1*X1 + beta2*X2) / (1 + exp(beta0 + beta1*X1 + beta2*X2))

# Simulating binary outcome variable.
Y <- rbinom(n=num_obs, size=1, prob=prob)

data <- data.frame(X1, X2, Y)
X <- as.matrix(data[1:ncol(data)-1])
Y <- as.matrix(data[,ncol(data)])
```

# Running the functions of BackPropNN package.

```{r}
i <- 2 # number of input nodes
h <- 8 # number of hidden nodes
o <- 1 # number of output nodes
learning_rate <- 0.1 # The learning rate of the algorithm
activation_func <- "sigmoid" # the activation function
nn_model <- back_propagation_training(i, h, o, learning_rate, activation_func, data)
nn_model_nnet <- nnet::nnet(X,Y,size=h, trace=FALSE)
```

# Summarizing the results of nn_model.

```{r}
plot(nn_model)
summary(nn_model)
print(nn_model) # This will print the benchmark comparison for training part.

# Running the benchmark comparison for predicting part.
bench::mark("BackPropNN"=feed_forward(data, nn_model),
            "R nnet" = as.numeric(stats::predict(nn_model_nnet,X, type="raw")),
                    relative = TRUE, check = FALSE)
```

# Simulated data - AND data.

```{r}
set.seed(100)
num_obs <- 100000 # Number of observations
X1 <- sample(c(0,1),num_obs, replace = TRUE)
X2 <- sample(c(0,1),num_obs, replace = TRUE)
Y <- ifelse(X1==1 & X2==1, 1, 0)
data <- data.frame(X1,X2,Y)
X <- as.matrix(data[1:ncol(data)-1])
Y <- as.matrix(data[,ncol(data)])
```

# Running the functions of BackPropNN package.

```{r}
i <- 2 # number of input nodes
h <- 4 # number of hidden nodes
o <- 1 # number of output nodes
learning_rate <- 0.1 # The learning rate of the algorithm
activation_func <- "sigmoid" # the activation function
nn_model <- back_propagation_training(i, h, o, learning_rate, activation_func, data)
nn_model_nnet <- nnet::nnet(X,Y,size=h, trace=FALSE)
```

# Summarizing the results of nn_model.

```{r}
plot(nn_model)
summary(nn_model)
print(nn_model) # This will print the benchmark comparison for training part.

# Running the benchmark comparison for predicting part.
bench::mark("BackPropNN"=feed_forward(data, nn_model),
            "R nnet" = as.numeric(stats::predict(nn_model_nnet,X, type="raw")),
                    relative = TRUE, check = FALSE)
```

# Real data - IRIS data.

```{r}
data("iris") # Using iris data set.
data <- iris
data <- data[data$Species != "virginica", ] 
data$Species <- ifelse(data$Species=="setosa",0,1)
X <- as.matrix(data[1:ncol(data)-1])
Y <- as.matrix(data[,ncol(data)])
```

# Running the functions of BackPropNN package.

```{r}
set.seed(100)
i <- 4 # number of input nodes, i.e., number of X variables.
h <- 8 # number of hidden nodes
o <- 1 # number of output nodes
learning_rate <- 0.1 # The learning rate of the algorithm
activation_func <- "sigmoid" # the activation function
nn_model <- back_propagation_training(i, h, o, learning_rate, activation_func, data)
nn_model_nnet <- nnet::nnet(X,Y,size=h, trace=FALSE)
```

# Summarizing the results of nn_model.

```{r}
plot(nn_model)
summary(nn_model)
print(nn_model) # This will print the benchmark comparison for training part.

# Running the benchmark comparison for predicting part.
bench::mark("BackPropNN"=feed_forward(data, nn_model),
            "R nnet" = as.numeric(stats::predict(nn_model_nnet,X, type="raw")),
                    relative = TRUE, check = FALSE)
```

# Now, let's check if the Rcpp version of BackPropNN helps to improve the computational speed.

```{r}
# Setting up the simulated logistics data.

set.seed(100)
num_obs <- 10000 # Number of observations
beta0 <- -2.5; beta1 <- 0.02; beta2 <- 0.01 # Setting coefficients values for the logit function.
X1 <- runif(n=num_obs, min=18, max=60); X2 <- runif(n=num_obs, min=100, max=250) # Simulating the independent variables.
prob <- exp(beta0 + beta1*X1 + beta2*X2) / (1 + exp(beta0 + beta1*X1 + beta2*X2))
Y <- rbinom(n=num_obs, size=1, prob=prob) # Simulating binary outcome variable.
data <- data.frame(X1, X2, Y)
X <- as.matrix(data[1:ncol(data)-1])
Y <- as.matrix(data[,ncol(data)])

# Setting up NN model specifications.
i <- 2 # number of input nodes
h <- 8 # number of hidden nodes
o <- 1 # number of output nodes
learning_rate <- 0.1 # The learning rate of the algorithm
activation_func <- "sigmoid" # the activation function

# Running NN models using both versions.
nn_model_original <- back_propagation_training(i, h, o, learning_rate,
                                          activation_func, data)
nn_model_rcpp <- back_propagation_training_rcpp(i, h, o, learning_rate,
                                          activation_func, as.matrix(data))
nn_model_nnet <- nnet::nnet(X,Y,size=h, trace=FALSE)

# Running the benchmark comparison for training part.
bench::mark("Original"=back_propagation_training(i, h, o, learning_rate,
                                          activation_func, data),
            "R nnet"=nnet::nnet(X,Y,size=h, trace=FALSE),
            "Rcpp"=back_propagation_training_rcpp(i, h, o, learning_rate,
                                          activation_func, as.matrix(data)),
                    relative = TRUE, check = FALSE)

# Running the benchmark comparison for predicting part.
bench::mark("Original"=feed_forward(data, nn_model_original),
            "R nnet" = as.numeric(stats::predict(nn_model_nnet,X, type="raw")),
            "Rcpp"=feed_forward_rcpp(data, nn_model_rcpp),
                    relative = TRUE, check = FALSE)
```

```{r}
# Setting up the simulated AND data.

set.seed(100)
num_obs <- 100000 # Number of observations
X1 <- sample(c(0,1),num_obs, replace = TRUE)
X2 <- sample(c(0,1),num_obs, replace = TRUE)
Y <- ifelse(X1==1 & X2==1, 1, 0)
data <- data.frame(X1,X2,Y)
X <- as.matrix(data[1:ncol(data)-1])
Y <- as.matrix(data[,ncol(data)])

# Setting up NN model specifications.
i <- 2 # number of input nodes
h <- 4 # number of hidden nodes
o <- 1 # number of output nodes
learning_rate <- 0.1 # The learning rate of the algorithm
activation_func <- "sigmoid" # the activation function

# Running NN models using both versions.
nn_model_original <- back_propagation_training(i, h, o, learning_rate,
                                          activation_func, data)
nn_model_rcpp <- back_propagation_training_rcpp(i, h, o, learning_rate,
                                          activation_func, as.matrix(data))
nn_model_nnet <- nnet::nnet(X,Y,size=h, trace=FALSE)

# Running the benchmark comparison for training part.
bench::mark("Original"=back_propagation_training(i, h, o, learning_rate,
                                          activation_func, data),
            "R nnet"=nnet::nnet(X,Y,size=h, trace=FALSE),
            "Rcpp"=back_propagation_training_rcpp(i, h, o, learning_rate,
                                          activation_func, as.matrix(data)),
                    relative = TRUE, check = FALSE)

# Running the benchmark comparison for predicting part.
bench::mark("Original"=feed_forward(data, nn_model_original),
            "R nnet" = as.numeric(stats::predict(nn_model_nnet,X, type="raw")),
            "Rcpp"=feed_forward_rcpp(data, nn_model_rcpp),
                    relative = TRUE, check = FALSE)
```

```{r}
# Setting up the Real IRIS data.

data("iris") # Using iris data set.
data <- iris
data <- data[data$Species != "virginica", ] 
data$Species <- ifelse(data$Species=="setosa",0,1)
X <- as.matrix(data[1:ncol(data)-1])
Y <- as.matrix(data[,ncol(data)])

# Setting up NN model specifications.
i <- 4 # number of input nodes
h <- 8 # number of hidden nodes
o <- 1 # number of output nodes
learning_rate <- 0.1 # The learning rate of the algorithm
activation_func <- "sigmoid" # the activation function

# Running NN models using both versions.
nn_model_original <- back_propagation_training(i, h, o, learning_rate,
                                          activation_func, data)
nn_model_rcpp <- back_propagation_training_rcpp(i, h, o, learning_rate,
                                          activation_func, as.matrix(data))
nn_model_nnet <- nnet::nnet(X,Y,size=h, trace=FALSE)

# Running the benchmark comparison for training part.
bench::mark("Original"=back_propagation_training(i, h, o, learning_rate,
                                          activation_func, data),
            "R nnet"=nnet::nnet(X,Y,size=h, trace=FALSE),
            "Rcpp"=back_propagation_training_rcpp(i, h, o, learning_rate,
                                          activation_func, as.matrix(data)),
                    relative = TRUE, check = FALSE)

# Running the benchmark comparison for predicting part.
bench::mark("Original"=feed_forward(data, nn_model_original),
            "R nnet" = as.numeric(stats::predict(nn_model_nnet,X, type="raw")),
            "Rcpp"=feed_forward_rcpp(data, nn_model_rcpp),
                    relative = TRUE, check = FALSE)
```



