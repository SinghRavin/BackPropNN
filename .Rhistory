print(nn_model)
library(BackPropNN)
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
library(BackPropNN)
library(stagedtrees)
num_obs <- 10000 # Number of observations
X1 <- sample(c(0,1),num_obs, replace = TRUE)
X2 <- sample(c(0,1),num_obs, replace = TRUE)
Y <- ifelse(X1==0&X2==0, 0, 1)
data <- data.frame(X1,X2,Y)
# data_xor <- generate_xor_dataset(n = num_X, N = 10000, eps = 1.2)
# data_xor <- data_xor[, c(2:(num_X+1),1)]
# data_xor[colnames(data_xor)] <- sapply(data_xor[colnames(data_xor)],as.numeric)
set.seed(100)
i <- 2 # number of input nodes
h <- 4 # number of hidden nodes
o <- 1 # number of output nodes
learning_rate <- 0.1 # The learning rate of the algorithm
activation_func <- "sigmoid" # the activation function
nn_model <- back_propagation_training(i, h, o, learning_rate, activation_func, data)
plot(nn_model)
summary(nn_model)
print(nn_model)
library(BackPropNN)
install.packages("bench")
library(BackPropNN)
data("iris")
head(iris)
data("iris")
data <- iris
data$Species <- ifelse(data$Species=="setosa",0,
ifelse(data$Species=="versicolor",1,2))
data("iris")
data <- iris
data$Species <- ifelse(data$Species=="setosa",0,
ifelse(data$Species=="versicolor",1,2))
set.seed(100)
i <- 4 # number of input nodes
h <- 8 # number of hidden nodes
o <- 1 # number of output nodes
learning_rate <- 0.1 # The learning rate of the algorithm
activation_func <- "sigmoid" # the activation function
nn_model <- back_propagation_training(i, h, o, learning_rate, activation_func, data)
plot(nn_model)
summary(nn_model)
print(nn_model)
feed_forward(data,nn_model)
accuracy(data$Species, feed_forward(data,nn_model)$pred)
data$Species
data("iris")
data <- iris
data <- data[data$Species != "virginica", ]
data$Species <- ifelse(data$Species=="setosa",0,1)
dim(data)
View(data)
data("iris")
data <- iris
data <- data[data$Species != "virginica", ]
data$Species <- ifelse(data$Species=="setosa",0,1)
set.seed(100)
i <- 4 # number of input nodes, i.e., number of X variables.
h <- 8 # number of hidden nodes
o <- 1 # number of output nodes
learning_rate <- 0.1 # The learning rate of the algorithm
activation_func <- "sigmoid" # the activation function
nn_model <- back_propagation_training(i, h, o, learning_rate, activation_func, data)
plot(nn_model)
summary(nn_model)
print(nn_model)
feed_forward(data,nn_model)$pred == data$Species
feed_forward(data,nn_model)$pred
data$Species
pROC::roc(data$Species,feed_forward(data,nn_model)$pred,
plot=TRUE, print.auc=TRUE, main="ROC curve")
library(BackPropNN)
library(BackPropNN)
data("iris")
data <- iris
data <- data[data$Species != "virginica", ]
data$Species <- ifelse(data$Species=="setosa",0,1)
set.seed(100)
i <- 4 # number of input nodes, i.e., number of X variables.
h <- 8 # number of hidden nodes
o <- 1 # number of output nodes
learning_rate <- 0.1 # The learning rate of the algorithm
activation_func <- "sigmoid" # the activation function
nn_model <- back_propagation_training(i, h, o, learning_rate, activation_func, data)
plot(nn_model)
summary(nn_model)
print(nn_model)
data
feed_forward(data,nn_model)
pROC::roc(runif(100),runif(100),
plot=TRUE, print.auc=TRUE, main="ROC curve")
pROC::roc(runif(100),sample(c(0,1),100, replace=TRUE),
plot=TRUE, print.auc=TRUE, main="ROC curve")
pROC::roc(sample(c(0,1),100),runif(100), replace=TRUE),
plot=TRUE, print.auc=TRUE, main="ROC curve")
pROC::roc(sample(c(0,1),100), replace=TRUE),runif(100),plot=TRUE, print.auc=TRUE, main="ROC curve")
pROC::roc(sample(c(0,1),100, replace=TRUE),runif(100),plot=TRUE, print.auc=TRUE, main="ROC curve")
pROC::roc(x$input_data[,ncol(x$input_data)],feed_forward(x$input_data,nn_model)$pred,
plot=TRUE, print.auc=TRUE, main="ROC curve")
sample(c(0,1),100, replace=TRUE)
runif(100)
pROC::roc(x$input_data[,ncol(x$input_data)],feed_forward(x$input_data,nn_model)$pred,
plot=TRUE, print.auc=TRUE, main="ROC curve")
pROC::roc(sample(c(0,1),100, replace=TRUE),runif(100),plot=TRUE, print.auc=TRUE, main="ROC curve")
pROC::roc(sample(c(0,1),100, replace=TRUE),runif(100),plot=TRUE, print.auc=TRUE, main="ROC curve")
pROC::roc(sample(c(0,1),100, replace=TRUE),runif(100),plot=TRUE, print.auc=TRUE, main="ROC curve")
pROC::roc(runif(100),sample(c(0,1),100, replace=TRUE),plot=TRUE, print.auc=TRUE, main="ROC curve")
library(BackPropNN)
library(BackPropNN)
data("iris")
data <- iris
data <- data[data$Species != "virginica", ]
data$Species <- ifelse(data$Species=="setosa",0,1)
set.seed(100)
i <- 4 # number of input nodes, i.e., number of X variables.
h <- 8 # number of hidden nodes
o <- 1 # number of output nodes
learning_rate <- 0.1 # The learning rate of the algorithm
activation_func <- "sigmoid" # the activation function
nn_model <- back_propagation_training(i, h, o, learning_rate, activation_func, data)
plot(nn_model)
summary(nn_model)
print(nn_model)
input_data <- data
input_data[,ncol(input_data)]
library(BackPropNN)
library(BackPropNN)
data("iris")
data <- iris
data <- data[data$Species != "virginica", ]
data$Species <- ifelse(data$Species=="setosa",0,1)
View(data)
sapply(data,class)
X = as.matrix(data[1:ncol(data)-1])
View(X)
Y = as.matrix(data[,ncol(data)])
View(Y)
set.seed(100)
i <- 4 # number of input nodes, i.e., number of X variables.
h <- 8 # number of hidden nodes
o <- 1 # number of output nodes
learning_rate <- 0.1 # The learning rate of the algorithm
activation_func <- "sigmoid" # the activation function
nn_model <- back_propagation_training(i, h, o, learning_rate, activation_func, data)
plot(nn_model)
summary(nn_model)
print(nn_model)
vec1 <- data$Species
vec2 <- feed_forward(data,nn_model)
roc(vec1,vec2)
pROC:roc(vec1,vec2)
library(pROC)
pROC:roc(vec1,vec2)
pROC::roc(vec1,vec2)
class(vec1)
class(vec2)
vec2 <- feed_forward(data,nn_model) $pred
roc(vec1,vec2)
roc(vec2,vec1)
vec1
vec2
num_obs <- 10000 # Number of observations
X1 <- sample(c(0,1),num_obs, replace = TRUE)
X2 <- sample(c(0,1),num_obs, replace = TRUE)
Y <- ifelse(X1==0 & X2==0, 0, 1)
data <- data.frame(X1,X2,Y)
set.seed(100)
i <- 2 # number of input nodes
h <- 4 # number of hidden nodes
o <- 1 # number of output nodes
learning_rate <- 0.1 # The learning rate of the algorithm
activation_func <- "sigmoid" # the activation function
nn_model <- back_propagation_training(i, h, o, learning_rate, activation_func, data)
plot(nn_model)
summary(nn_model)
print(nn_model)
vec1 <- data$Y
vec2 <- feed_forward(data,nn_model)$pred
vec1
vec2
min(vec2)
max(vec2)
summary(vec2)
auc(vec1,vec2)
library(BackPropNN)
library(BackPropNN)
num_obs <- 10000 # Number of observations
X1 <- sample(c(0,1),num_obs, replace = TRUE)
X2 <- sample(c(0,1),num_obs, replace = TRUE)
Y <- ifelse(X1==0 & X2==0, 0, 1)
data <- data.frame(X1,X2,Y)
set.seed(100)
i <- 2 # number of input nodes
h <- 4 # number of hidden nodes
o <- 1 # number of output nodes
learning_rate <- 0.1 # The learning rate of the algorithm
activation_func <- "sigmoid" # the activation function
nn_model <- back_propagation_training(i, h, o, learning_rate, activation_func, data)
plot(nn_model)
library(BackPropNN)
library(BackPropNN)
num_obs <- 10000 # Number of observations
X1 <- sample(c(0,1),num_obs, replace = TRUE)
X2 <- sample(c(0,1),num_obs, replace = TRUE)
Y <- ifelse(X1==0 & X2==0, 0, 1)
data <- data.frame(X1,X2,Y)
set.seed(100)
i <- 2 # number of input nodes
h <- 4 # number of hidden nodes
o <- 1 # number of output nodes
learning_rate <- 0.1 # The learning rate of the algorithm
activation_func <- "sigmoid" # the activation function
nn_model <- back_propagation_training(i, h, o, learning_rate, activation_func, data)
plot(nn_model)
summary(nn_model)
print(nn_model)
library(BackPropNN)
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
library(BackPropNN)
num_obs <- 10000 # Number of observations
X1 <- sample(c(0,1),num_obs, replace = TRUE)
X2 <- sample(c(0,1),num_obs, replace = TRUE)
Y <- ifelse(X1==0 & X2==0, 0, 1)
data <- data.frame(X1,X2,Y)
set.seed(100)
i <- 2 # number of input nodes
h <- 4 # number of hidden nodes
o <- 1 # number of output nodes
learning_rate <- 0.1 # The learning rate of the algorithm
activation_func <- "sigmoid" # the activation function
nn_model <- back_propagation_training(i, h, o, learning_rate, activation_func, data)
plot(nn_model)
summary(nn_model)
print(nn_model)
data
nn_R <- nnet::nnet(X,Y,size=x$num_nodes[2], trace=FALSE)
data <- x$input_data
X = as.matrix(data[1:ncol(data)-1])
Y = as.matrix(data[,ncol(data)])
nn_R <- nnet::nnet(X,Y,size=x$num_nodes[2], trace=FALSE)
nn_R_pred <- as.numeric(stats::predict(nn_R,X, type="raw"))
X = as.matrix(data[1:ncol(data)-1])
Y = as.matrix(data[,ncol(data)])
nn_R <- nnet::nnet(X,Y,size=x$num_nodes[2], trace=FALSE)
nn_R_pred <- as.numeric(stats::predict(nn_R,X, type="raw"))
X
nn_R <- nnet::nnet(X,Y,size=4, trace=FALSE)
nn_R_pred <- as.numeric(stats::predict(nn_R,X, type="raw"))
nn_R_pred == Y
Y
nn_pred
nn_R_pred
data("iris")
data <- iris
data <- data[data$Species != "virginica", ]
data$Species <- ifelse(data$Species=="setosa",0,1)
set.seed(100)
i <- 4 # number of input nodes, i.e., number of X variables.
h <- 8 # number of hidden nodes
o <- 1 # number of output nodes
learning_rate <- 0.1 # The learning rate of the algorithm
activation_func <- "sigmoid" # the activation function
nn_model <- back_propagation_training(i, h, o, learning_rate, activation_func, data)
plot(nn_model)
summary(nn_model)
print(nn_model)
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
library(BackPropNN)
num_obs <- 10000 # Number of observations
X1 <- sample(c(0,1),num_obs, replace = TRUE)
X2 <- sample(c(0,1),num_obs, replace = TRUE)
Y <- ifelse(X1==0 & X2==0, 0, 1)
data <- data.frame(X1,X2,Y)
set.seed(100)
i <- 2 # number of input nodes
h <- 4 # number of hidden nodes
o <- 1 # number of output nodes
learning_rate <- 0.1 # The learning rate of the algorithm
activation_func <- "sigmoid" # the activation function
nn_model <- back_propagation_training(i, h, o, learning_rate, activation_func, data)
plot(nn_model)
summary(nn_model)
print(nn_model)
dim(data)
X = as.matrix(data[1:ncol(data)-1])
Y = as.matrix(data[,ncol(data)])
nn_R <- nnet::nnet(X,Y,size=4, trace=FALSE)
nn_R_pred <- as.numeric(stats::predict(nn_R,X, type="raw"))
auc(Y,nn_R_pred)
class(Y)
auc(data$Y,nn_R_pred)
nn_model$num_nodes
nn_model$num_nodes[2]
nn_model$num_nodes[2]*4
dim(data)
data[,ncol(data)]
data[ncol(data)]
class(data[ncol(data)])
class(data[,ncol(data)])
data[ncol(data)] = data[,ncol(data)]
data[ncol(data)] == data[,ncol(data)]
sum(data[ncol(data)] == data[,ncol(data)])
nn_R_pred
library(BackPropNN)
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
library(BackPropNN)
num_obs <- 10000 # Number of observations
X1 <- sample(c(0,1),num_obs, replace = TRUE)
X2 <- sample(c(0,1),num_obs, replace = TRUE)
Y <- ifelse(X1==0 & X2==0, 0, 1)
data <- data.frame(X1,X2,Y)
set.seed(100)
i <- 2 # number of input nodes
h <- 4 # number of hidden nodes
o <- 1 # number of output nodes
learning_rate <- 0.1 # The learning rate of the algorithm
activation_func <- "sigmoid" # the activation function
nn_model <- back_propagation_training(i, h, o, learning_rate, activation_func, data)
plot(nn_model)
summary(nn_model)
print(nn_model)
X = as.matrix(data[1:ncol(data)-1])
Y = as.matrix(data[,ncol(data)])
nn_R <- nnet::nnet(X,Y,size=4, trace=FALSE)
nn_R_pred <- stats::predict(nn_R,X)
nn_R_mse <- mean((Y - nn_R_pred)^2)
nn_R_mse
auc(data$Y,nn_R_pred)
class(data$Y)
class(nn_R_pred)
auc(data$Y,as.numeric(nn_R_pred))
data("iris")
data <- iris
data <- data[data$Species != "virginica", ]
data$Species <- ifelse(data$Species=="setosa",0,1)
set.seed(100)
i <- 4 # number of input nodes, i.e., number of X variables.
h <- 8 # number of hidden nodes
o <- 1 # number of output nodes
learning_rate <- 0.1 # The learning rate of the algorithm
activation_func <- "sigmoid" # the activation function
nn_model <- back_propagation_training(i, h, o, learning_rate, activation_func, data)
plot(nn_model)
summary(nn_model)
print(nn_model)
nn_model$num_nodes
X = as.matrix(data[1:ncol(data)-1])
Y = as.matrix(data[,ncol(data)])
X
Y
nn_R <- nnet::nnet(X,Y,size=8, trace=FALSE)
nn_R_pred <- as.numeric(stats::predict(nn_R,X))
auc(data$Species,nn_R_pred)
nn_R_mse <- mean((Y - nn_R_pred)^2)
nn_R_mse
nn_R_pred
mean((data$Species - feed_forward(data,nn_model)$pred)^2)
library(BackPropNN)
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
library(BackPropNN)
num_obs <- 10000 # Number of observations
X1 <- sample(c(0,1),num_obs, replace = TRUE)
X2 <- sample(c(0,1),num_obs, replace = TRUE)
Y <- ifelse(X1==0 & X2==0, 0, 1)
data <- data.frame(X1,X2,Y)
set.seed(100)
i <- 2 # number of input nodes
h <- 4 # number of hidden nodes
o <- 1 # number of output nodes
learning_rate <- 0.1 # The learning rate of the algorithm
activation_func <- "sigmoid" # the activation function
nn_model <- back_propagation_training(i, h, o, learning_rate, activation_func, data)
plot(nn_model)
summary(nn_model)
print(nn_model)
dim(data)
View(data)
X = as.matrix(data[1:ncol(data)-1])
Y = as.matrix(data[,ncol(data)])
nn_model$num_nodes
nn_R <- nnet::nnet(X,Y,size=nn_model$num_nodes[2], trace=FALSE)
nn_R_pred <- as.numeric(stats::predict(nn_R,X))
pROC::roc(data[,ncol(data)],feed_forward(data,nn_model)$pred,
plot=TRUE, print.auc=TRUE, main="ROC curve - BackPropNN")
pROC::roc(data[,ncol(data)],nn_R_pred,
plot=TRUE, print.auc=TRUE, main="ROC curve - R nnet")
sum(nn_R_pred == feed_forward(data,nn_model)$pred)
nn_R_pred
feed_forward(data,nn_model)
library(BackPropNN)
matrix(0.01,2,1)
matrix(0.01,2,2)
matrix(0.01,3,2)
matrix(0.01,nrow=3,ncol=2)
matrix(0.01,nrow=2,ncol=3)
matrix(3,2)
matrix(3,2,2)
library(BackPropNN)
library(BackPropNN)
num_obs <- 10000 # Number of observations
X1 <- sample(c(0,1),num_obs, replace = TRUE)
X2 <- sample(c(0,1),num_obs, replace = TRUE)
Y <- ifelse(X1==1 & X2==1, 1, 0)
data <- data.frame(X1,X2,Y)
set.seed(100)
i <- 2 # number of input nodes
h <- 4 # number of hidden nodes
o <- 1 # number of output nodes
learning_rate <- 0.1 # The learning rate of the algorithm
activation_func <- "sigmoid" # the activation function
nn_model <- back_propagation_training(i, h, o, learning_rate, activation_func, data)
plot(nn_model)
summary(nn_model)
print(nn_model)
library(BackPropNN)
getwd()
setwd("C:/Users/u1374012/Documents/Everything/U_PhD/Spring2023/PHS_7045")
install.packages("BackPropNN_0.1-0.tar.gz")
library(BackPropNN)
set.seed(100)
data <- data.frame(X1 = 1:100, X2 = 2:101, Y = sample(c(0,1), 100, replace=TRUE))
nn_model <- back_propagation_training(i=2, h=2, o=1, learning_rate=0.01,
activation_func="sigmoid", data=data)
usethis::use_github_action_check_standard()
library(BackPropNN)
n <- 10000
beta0 <- -1.6
beta1 <- 0.03
x <- runif(n=n, min=18, max=60)
pi_x <- exp(beta0 + beta1 * x) / (1 + exp(beta0 + beta1 * x))
y <- rbinom(n=length(x), size=1, prob=pi_x)
data <- data.frame(x, pi_x, y)
names(data) <- c("age", "pi", "y")
View(data)
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
library(BackPropNN)
num_obs <- 10000 # Number of observations
# Setting coefficients values for the logit function.
beta0 <- -1
beta1 <- 0.05
beta2 <- 0.1
# Simulating the independent variables.
X1 <- runif(n=num_obs, min=18, max=60)
X2 <- runif(n=num_obs, min=100, max=250)
prob <- exp(beta0 + beta1*X1 + beta2*X2) / (1 + exp(beta0 + beta1*X1 + beta2*X2))
# Generating binary outcome variable.
Y <- rbinom(n=num_obs, size=1, prob=prob)
data <- data.frame(X1, X2, Y)
View(data)
unique(data$Y)
rbinom(n=num_obs, size=1, prob=prob)
prob
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
library(BackPropNN)
num_obs <- 10000 # Number of observations
# Setting coefficients values for the logit function.
beta0 <- -1
beta1 <- 0.05
beta2 <- 0.01
# Simulating the independent variables.
X1 <- runif(n=num_obs, min=18, max=60)
X2 <- runif(n=num_obs, min=100, max=250)
prob <- exp(beta0 + beta1*X1 + beta2*X2) / (1 + exp(beta0 + beta1*X1 + beta2*X2))
# Generating binary outcome variable.
Y <- rbinom(n=num_obs, size=1, prob=prob)
data <- data.frame(X1, X2, Y)
unique(data$Y)
sum(data$Y==1)
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
library(BackPropNN)
num_obs <- 10000 # Number of observations
# Setting coefficients values for the logit function.
beta0 <- -2.5
beta1 <- 0.02
beta2 <- 0.01
# Simulating the independent variables.
X1 <- runif(n=num_obs, min=18, max=60)
X2 <- runif(n=num_obs, min=100, max=250)
prob <- exp(beta0 + beta1*X1 + beta2*X2) / (1 + exp(beta0 + beta1*X1 + beta2*X2))
# Generating binary outcome variable.
Y <- rbinom(n=num_obs, size=1, prob=prob)
data <- data.frame(X1, X2, Y)
sum(data$Y==1)
source("~/.active-rstudio-document", echo=TRUE)
install.packages("BackPropNN")
