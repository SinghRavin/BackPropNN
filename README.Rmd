---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# BackPropNN

<!-- badges: start -->
[![R-CMD-check](https://github.com/SinghRavin/BackPropNN/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/SinghRavin/BackPropNN/actions/workflows/R-CMD-check.yaml)
<!-- badges: end -->

The neural network algorithm trained by the process of backpropagation is implemented. The stochastic gradient descent (SGD) is adopted for this algorithm i.e., the weight and bias matrices are updated after learning from the errors of each dataset point one by one. In this package, 3 layer (input, one hidden and output) neural network is considered. The mathematical equation involved are given below,

FeedForward:
[H] = sigma([W_IH].[I] + [B_H])
[O] = sigma([W_HO].[H] + [B_O])

Backpropagation:
[delta_W_HO] = (learning_rate)[Output_Errors]x[O(1-O)].[H_tranpose]
[delta_W_IH] = (learning_rate)[Hiddden_Errors]x[H(1-H)].[I_tranpose]
[delta_B_O] = (learning_rate)[Output_Errors]x[O(1-O)]
[delta_B_H] = (learning_rate)[Hidden_Errors]x[H(1-H)]

Where, [] represents the matrix, x represents the Hadamard multiplication (elementwise), and . represents the usual matrix multiplication.
H represents hidden matrix, O represents output matrix, W_IH represents weight matrix between input layer and hidden layer, W_HO represents weight matrix between hidden layer and output layer, B_H represents bias matrix for hidden layer, B_O represents bias matrix for output layer.
The chosen activation function are Sigmoid or ReLU.

## Installation

You can install the development version of BackPropNN like so:

``` r
install.packages("BackPropNN")
```

## Example

This is a basic example which shows you how to solve a common problem:

```{r example}
library(BackPropNN)
num_obs <- 1000
X1=sample(c(0,1),num_obs, replace = TRUE)
X2=sample(c(0,1),num_obs, replace = TRUE)
data <- data.frame(X1,X2,Y=ifelse(X1==0 & X2==0, 0, 1)) # Setting up the data
i <- 2 # number of input nodes, which must be equal to the number of X variables.
h <- 4 # number of hidden nodes
o <- 1 # number of output nodes
learning_rate <- 0.1 # The learning rate of the algorithm
activation_func <- "sigmoid" # the activation function
nn_model <- back_propagation_training(i, h, o, learning_rate, activation_func, data)

# Plot (ROC-AUC curve), summary and print function.
plot(nn_model)
summary(nn_model)
print(nn_model)
```
