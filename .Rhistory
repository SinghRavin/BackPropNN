comment = "#>"
)
library(BackPropNN)
library(stagedtrees)
num_obs <- 1000 # Number of observations
X1 <- sample(c(0,1),num_obs, replace = TRUE)
X2 <- sample(c(0,1),num_obs, replace = TRUE)
Y <- ifelse(X1==0&X2==0, 0, 1)
data <- data.frame(X1,X2,Y)
# data_xor <- generate_xor_dataset(n = num_X, N = 10000, eps = 1.2)
# data_xor <- data_xor[, c(2:(num_X+1),1)]
# data_xor[colnames(data_xor)] <- sapply(data_xor[colnames(data_xor)],as.numeric)
set.seed(100)
i <- 2 # number of input nodes
h <- 5 # number of hidden nodes
o <- 1 # number of output nodes
learning_rate <- 0.1 # The learning rate of the algorithm
activation_func <- "ReLU" # the activation function
data <- data_xor
nn_model <- back_propagation_training(i, h, o, learning_rate, activation_func, data)
set.seed(100)
i <- 2 # number of input nodes
h <- 2 # number of hidden nodes
o <- 1 # number of output nodes
learning_rate <- 0.1 # The learning rate of the algorithm
activation_func <- "ReLU" # the activation function
data <- data_xor
nn_model <- back_propagation_training(i, h, o, learning_rate, activation_func, data)
class(data$X1)
data <- data.frame(X1,X2,Y)
class(data$X1)
class(data$X2)
class(data$Y)
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
library(BackPropNN)
library(stagedtrees)
num_obs <- 1000 # Number of observations
X1 <- sample(c(0,1),num_obs, replace = TRUE)
X2 <- sample(c(0,1),num_obs, replace = TRUE)
Y <- ifelse(X1==0&X2==0, 0, 1)
data <- data.frame(X1,X2,Y)
# data_xor <- generate_xor_dataset(n = num_X, N = 10000, eps = 1.2)
# data_xor <- data_xor[, c(2:(num_X+1),1)]
# data_xor[colnames(data_xor)] <- sapply(data_xor[colnames(data_xor)],as.numeric)
set.seed(100)
i <- 2 # number of input nodes
h <- 4 # number of hidden nodes
o <- 1 # number of output nodes
learning_rate <- 0.1 # The learning rate of the algorithm
activation_func <- "ReLU" # the activation function
nn_model <- back_propagation_training(i, h, o, learning_rate, activation_func, data)
plot(nn_model)
summary(nn_model)
print(nn_model)
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
library(BackPropNN)
library(stagedtrees)
num_obs <- 1000 # Number of observations
X1 <- sample(c(0,1),num_obs, replace = TRUE)
X2 <- sample(c(0,1),num_obs, replace = TRUE)
Y <- ifelse(X1==0&X2==0, 0, 1)
data <- data.frame(X1,X2,Y)
# data_xor <- generate_xor_dataset(n = num_X, N = 10000, eps = 1.2)
# data_xor <- data_xor[, c(2:(num_X+1),1)]
# data_xor[colnames(data_xor)] <- sapply(data_xor[colnames(data_xor)],as.numeric)
set.seed(100)
i <- 2 # number of input nodes
h <- 4 # number of hidden nodes
o <- 1 # number of output nodes
learning_rate <- 0.1 # The learning rate of the algorithm
activation_func <- "sigmoid" # the activation function
nn_model <- back_propagation_training(i, h, o, learning_rate, activation_func, data)
plot(nn_model)
summary(nn_model)
print(nn_model)
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
library(BackPropNN)
library(stagedtrees)
num_obs <- 10000 # Number of observations
X1 <- sample(c(0,1),num_obs, replace = TRUE)
X2 <- sample(c(0,1),num_obs, replace = TRUE)
Y <- ifelse(X1==0&X2==0, 0, 1)
data <- data.frame(X1,X2,Y)
# data_xor <- generate_xor_dataset(n = num_X, N = 10000, eps = 1.2)
# data_xor <- data_xor[, c(2:(num_X+1),1)]
# data_xor[colnames(data_xor)] <- sapply(data_xor[colnames(data_xor)],as.numeric)
set.seed(100)
i <- 2 # number of input nodes
h <- 4 # number of hidden nodes
o <- 1 # number of output nodes
learning_rate <- 0.1 # The learning rate of the algorithm
activation_func <- "sigmoid" # the activation function
nn_model <- back_propagation_training(i, h, o, learning_rate, activation_func, data)
plot(nn_model)
summary(nn_model)
print(nn_model)
library(BackPropNN)
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
library(BackPropNN)
library(stagedtrees)
num_obs <- 10000 # Number of observations
X1 <- sample(c(0,1),num_obs, replace = TRUE)
X2 <- sample(c(0,1),num_obs, replace = TRUE)
Y <- ifelse(X1==0&X2==0, 0, 1)
data <- data.frame(X1,X2,Y)
# data_xor <- generate_xor_dataset(n = num_X, N = 10000, eps = 1.2)
# data_xor <- data_xor[, c(2:(num_X+1),1)]
# data_xor[colnames(data_xor)] <- sapply(data_xor[colnames(data_xor)],as.numeric)
set.seed(100)
i <- 2 # number of input nodes
h <- 4 # number of hidden nodes
o <- 1 # number of output nodes
learning_rate <- 0.1 # The learning rate of the algorithm
activation_func <- "sigmoid" # the activation function
nn_model <- back_propagation_training(i, h, o, learning_rate, activation_func, data)
plot(nn_model)
summary(nn_model)
print(nn_model)
library(BackPropNN)
install.packages("bench")
library(BackPropNN)
data("iris")
head(iris)
data("iris")
data <- iris
data$Species <- ifelse(data$Species=="setosa",0,
ifelse(data$Species=="versicolor",1,2))
data("iris")
data <- iris
data$Species <- ifelse(data$Species=="setosa",0,
ifelse(data$Species=="versicolor",1,2))
set.seed(100)
i <- 4 # number of input nodes
h <- 8 # number of hidden nodes
o <- 1 # number of output nodes
learning_rate <- 0.1 # The learning rate of the algorithm
activation_func <- "sigmoid" # the activation function
nn_model <- back_propagation_training(i, h, o, learning_rate, activation_func, data)
plot(nn_model)
summary(nn_model)
print(nn_model)
feed_forward(data,nn_model)
accuracy(data$Species, feed_forward(data,nn_model)$pred)
data$Species
data("iris")
data <- iris
data <- data[data$Species != "virginica", ]
data$Species <- ifelse(data$Species=="setosa",0,1)
dim(data)
View(data)
data("iris")
data <- iris
data <- data[data$Species != "virginica", ]
data$Species <- ifelse(data$Species=="setosa",0,1)
set.seed(100)
i <- 4 # number of input nodes, i.e., number of X variables.
h <- 8 # number of hidden nodes
o <- 1 # number of output nodes
learning_rate <- 0.1 # The learning rate of the algorithm
activation_func <- "sigmoid" # the activation function
nn_model <- back_propagation_training(i, h, o, learning_rate, activation_func, data)
plot(nn_model)
summary(nn_model)
print(nn_model)
feed_forward(data,nn_model)$pred == data$Species
feed_forward(data,nn_model)$pred
data$Species
pROC::roc(data$Species,feed_forward(data,nn_model)$pred,
plot=TRUE, print.auc=TRUE, main="ROC curve")
library(BackPropNN)
library(BackPropNN)
data("iris")
data <- iris
data <- data[data$Species != "virginica", ]
data$Species <- ifelse(data$Species=="setosa",0,1)
set.seed(100)
i <- 4 # number of input nodes, i.e., number of X variables.
h <- 8 # number of hidden nodes
o <- 1 # number of output nodes
learning_rate <- 0.1 # The learning rate of the algorithm
activation_func <- "sigmoid" # the activation function
nn_model <- back_propagation_training(i, h, o, learning_rate, activation_func, data)
plot(nn_model)
summary(nn_model)
print(nn_model)
data
feed_forward(data,nn_model)
pROC::roc(runif(100),runif(100),
plot=TRUE, print.auc=TRUE, main="ROC curve")
pROC::roc(runif(100),sample(c(0,1),100, replace=TRUE),
plot=TRUE, print.auc=TRUE, main="ROC curve")
pROC::roc(sample(c(0,1),100),runif(100), replace=TRUE),
plot=TRUE, print.auc=TRUE, main="ROC curve")
pROC::roc(sample(c(0,1),100), replace=TRUE),runif(100),plot=TRUE, print.auc=TRUE, main="ROC curve")
pROC::roc(sample(c(0,1),100, replace=TRUE),runif(100),plot=TRUE, print.auc=TRUE, main="ROC curve")
pROC::roc(x$input_data[,ncol(x$input_data)],feed_forward(x$input_data,nn_model)$pred,
plot=TRUE, print.auc=TRUE, main="ROC curve")
sample(c(0,1),100, replace=TRUE)
runif(100)
pROC::roc(x$input_data[,ncol(x$input_data)],feed_forward(x$input_data,nn_model)$pred,
plot=TRUE, print.auc=TRUE, main="ROC curve")
pROC::roc(sample(c(0,1),100, replace=TRUE),runif(100),plot=TRUE, print.auc=TRUE, main="ROC curve")
pROC::roc(sample(c(0,1),100, replace=TRUE),runif(100),plot=TRUE, print.auc=TRUE, main="ROC curve")
pROC::roc(sample(c(0,1),100, replace=TRUE),runif(100),plot=TRUE, print.auc=TRUE, main="ROC curve")
pROC::roc(runif(100),sample(c(0,1),100, replace=TRUE),plot=TRUE, print.auc=TRUE, main="ROC curve")
library(BackPropNN)
library(BackPropNN)
data("iris")
data <- iris
data <- data[data$Species != "virginica", ]
data$Species <- ifelse(data$Species=="setosa",0,1)
set.seed(100)
i <- 4 # number of input nodes, i.e., number of X variables.
h <- 8 # number of hidden nodes
o <- 1 # number of output nodes
learning_rate <- 0.1 # The learning rate of the algorithm
activation_func <- "sigmoid" # the activation function
nn_model <- back_propagation_training(i, h, o, learning_rate, activation_func, data)
plot(nn_model)
summary(nn_model)
print(nn_model)
input_data <- data
input_data[,ncol(input_data)]
library(BackPropNN)
library(BackPropNN)
data("iris")
data <- iris
data <- data[data$Species != "virginica", ]
data$Species <- ifelse(data$Species=="setosa",0,1)
View(data)
sapply(data,class)
X = as.matrix(data[1:ncol(data)-1])
View(X)
Y = as.matrix(data[,ncol(data)])
View(Y)
set.seed(100)
i <- 4 # number of input nodes, i.e., number of X variables.
h <- 8 # number of hidden nodes
o <- 1 # number of output nodes
learning_rate <- 0.1 # The learning rate of the algorithm
activation_func <- "sigmoid" # the activation function
nn_model <- back_propagation_training(i, h, o, learning_rate, activation_func, data)
plot(nn_model)
summary(nn_model)
print(nn_model)
vec1 <- data$Species
vec2 <- feed_forward(data,nn_model)
roc(vec1,vec2)
pROC:roc(vec1,vec2)
library(pROC)
pROC:roc(vec1,vec2)
pROC::roc(vec1,vec2)
class(vec1)
class(vec2)
vec2 <- feed_forward(data,nn_model) $pred
roc(vec1,vec2)
roc(vec2,vec1)
vec1
vec2
num_obs <- 10000 # Number of observations
X1 <- sample(c(0,1),num_obs, replace = TRUE)
X2 <- sample(c(0,1),num_obs, replace = TRUE)
Y <- ifelse(X1==0 & X2==0, 0, 1)
data <- data.frame(X1,X2,Y)
set.seed(100)
i <- 2 # number of input nodes
h <- 4 # number of hidden nodes
o <- 1 # number of output nodes
learning_rate <- 0.1 # The learning rate of the algorithm
activation_func <- "sigmoid" # the activation function
nn_model <- back_propagation_training(i, h, o, learning_rate, activation_func, data)
plot(nn_model)
summary(nn_model)
print(nn_model)
vec1 <- data$Y
vec2 <- feed_forward(data,nn_model)$pred
vec1
vec2
min(vec2)
max(vec2)
summary(vec2)
auc(vec1,vec2)
library(BackPropNN)
library(BackPropNN)
num_obs <- 10000 # Number of observations
X1 <- sample(c(0,1),num_obs, replace = TRUE)
X2 <- sample(c(0,1),num_obs, replace = TRUE)
Y <- ifelse(X1==0 & X2==0, 0, 1)
data <- data.frame(X1,X2,Y)
set.seed(100)
i <- 2 # number of input nodes
h <- 4 # number of hidden nodes
o <- 1 # number of output nodes
learning_rate <- 0.1 # The learning rate of the algorithm
activation_func <- "sigmoid" # the activation function
nn_model <- back_propagation_training(i, h, o, learning_rate, activation_func, data)
plot(nn_model)
library(BackPropNN)
library(BackPropNN)
num_obs <- 10000 # Number of observations
X1 <- sample(c(0,1),num_obs, replace = TRUE)
X2 <- sample(c(0,1),num_obs, replace = TRUE)
Y <- ifelse(X1==0 & X2==0, 0, 1)
data <- data.frame(X1,X2,Y)
set.seed(100)
i <- 2 # number of input nodes
h <- 4 # number of hidden nodes
o <- 1 # number of output nodes
learning_rate <- 0.1 # The learning rate of the algorithm
activation_func <- "sigmoid" # the activation function
nn_model <- back_propagation_training(i, h, o, learning_rate, activation_func, data)
plot(nn_model)
summary(nn_model)
print(nn_model)
library(BackPropNN)
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
library(BackPropNN)
num_obs <- 10000 # Number of observations
X1 <- sample(c(0,1),num_obs, replace = TRUE)
X2 <- sample(c(0,1),num_obs, replace = TRUE)
Y <- ifelse(X1==0 & X2==0, 0, 1)
data <- data.frame(X1,X2,Y)
set.seed(100)
i <- 2 # number of input nodes
h <- 4 # number of hidden nodes
o <- 1 # number of output nodes
learning_rate <- 0.1 # The learning rate of the algorithm
activation_func <- "sigmoid" # the activation function
nn_model <- back_propagation_training(i, h, o, learning_rate, activation_func, data)
plot(nn_model)
summary(nn_model)
print(nn_model)
data
nn_R <- nnet::nnet(X,Y,size=x$num_nodes[2], trace=FALSE)
data <- x$input_data
X = as.matrix(data[1:ncol(data)-1])
Y = as.matrix(data[,ncol(data)])
nn_R <- nnet::nnet(X,Y,size=x$num_nodes[2], trace=FALSE)
nn_R_pred <- as.numeric(stats::predict(nn_R,X, type="raw"))
X = as.matrix(data[1:ncol(data)-1])
Y = as.matrix(data[,ncol(data)])
nn_R <- nnet::nnet(X,Y,size=x$num_nodes[2], trace=FALSE)
nn_R_pred <- as.numeric(stats::predict(nn_R,X, type="raw"))
X
nn_R <- nnet::nnet(X,Y,size=4, trace=FALSE)
nn_R_pred <- as.numeric(stats::predict(nn_R,X, type="raw"))
nn_R_pred == Y
Y
nn_pred
nn_R_pred
data("iris")
data <- iris
data <- data[data$Species != "virginica", ]
data$Species <- ifelse(data$Species=="setosa",0,1)
set.seed(100)
i <- 4 # number of input nodes, i.e., number of X variables.
h <- 8 # number of hidden nodes
o <- 1 # number of output nodes
learning_rate <- 0.1 # The learning rate of the algorithm
activation_func <- "sigmoid" # the activation function
nn_model <- back_propagation_training(i, h, o, learning_rate, activation_func, data)
plot(nn_model)
summary(nn_model)
print(nn_model)
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
library(BackPropNN)
num_obs <- 10000 # Number of observations
X1 <- sample(c(0,1),num_obs, replace = TRUE)
X2 <- sample(c(0,1),num_obs, replace = TRUE)
Y <- ifelse(X1==0 & X2==0, 0, 1)
data <- data.frame(X1,X2,Y)
set.seed(100)
i <- 2 # number of input nodes
h <- 4 # number of hidden nodes
o <- 1 # number of output nodes
learning_rate <- 0.1 # The learning rate of the algorithm
activation_func <- "sigmoid" # the activation function
nn_model <- back_propagation_training(i, h, o, learning_rate, activation_func, data)
plot(nn_model)
summary(nn_model)
print(nn_model)
dim(data)
X = as.matrix(data[1:ncol(data)-1])
Y = as.matrix(data[,ncol(data)])
nn_R <- nnet::nnet(X,Y,size=4, trace=FALSE)
nn_R_pred <- as.numeric(stats::predict(nn_R,X, type="raw"))
auc(Y,nn_R_pred)
class(Y)
auc(data$Y,nn_R_pred)
nn_model$num_nodes
nn_model$num_nodes[2]
nn_model$num_nodes[2]*4
dim(data)
data[,ncol(data)]
data[ncol(data)]
class(data[ncol(data)])
class(data[,ncol(data)])
data[ncol(data)] = data[,ncol(data)]
data[ncol(data)] == data[,ncol(data)]
sum(data[ncol(data)] == data[,ncol(data)])
nn_R_pred
library(BackPropNN)
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
library(BackPropNN)
num_obs <- 10000 # Number of observations
X1 <- sample(c(0,1),num_obs, replace = TRUE)
X2 <- sample(c(0,1),num_obs, replace = TRUE)
Y <- ifelse(X1==0 & X2==0, 0, 1)
data <- data.frame(X1,X2,Y)
set.seed(100)
i <- 2 # number of input nodes
h <- 4 # number of hidden nodes
o <- 1 # number of output nodes
learning_rate <- 0.1 # The learning rate of the algorithm
activation_func <- "sigmoid" # the activation function
nn_model <- back_propagation_training(i, h, o, learning_rate, activation_func, data)
plot(nn_model)
summary(nn_model)
print(nn_model)
X = as.matrix(data[1:ncol(data)-1])
Y = as.matrix(data[,ncol(data)])
nn_R <- nnet::nnet(X,Y,size=4, trace=FALSE)
nn_R_pred <- stats::predict(nn_R,X)
nn_R_mse <- mean((Y - nn_R_pred)^2)
nn_R_mse
auc(data$Y,nn_R_pred)
class(data$Y)
class(nn_R_pred)
auc(data$Y,as.numeric(nn_R_pred))
data("iris")
data <- iris
data <- data[data$Species != "virginica", ]
data$Species <- ifelse(data$Species=="setosa",0,1)
set.seed(100)
i <- 4 # number of input nodes, i.e., number of X variables.
h <- 8 # number of hidden nodes
o <- 1 # number of output nodes
learning_rate <- 0.1 # The learning rate of the algorithm
activation_func <- "sigmoid" # the activation function
nn_model <- back_propagation_training(i, h, o, learning_rate, activation_func, data)
plot(nn_model)
summary(nn_model)
print(nn_model)
nn_model$num_nodes
X = as.matrix(data[1:ncol(data)-1])
Y = as.matrix(data[,ncol(data)])
X
Y
nn_R <- nnet::nnet(X,Y,size=8, trace=FALSE)
nn_R_pred <- as.numeric(stats::predict(nn_R,X))
auc(data$Species,nn_R_pred)
nn_R_mse <- mean((Y - nn_R_pred)^2)
nn_R_mse
nn_R_pred
mean((data$Species - feed_forward(data,nn_model)$pred)^2)
library(BackPropNN)
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
library(BackPropNN)
num_obs <- 10000 # Number of observations
X1 <- sample(c(0,1),num_obs, replace = TRUE)
X2 <- sample(c(0,1),num_obs, replace = TRUE)
Y <- ifelse(X1==0 & X2==0, 0, 1)
data <- data.frame(X1,X2,Y)
set.seed(100)
i <- 2 # number of input nodes
h <- 4 # number of hidden nodes
o <- 1 # number of output nodes
learning_rate <- 0.1 # The learning rate of the algorithm
activation_func <- "sigmoid" # the activation function
nn_model <- back_propagation_training(i, h, o, learning_rate, activation_func, data)
plot(nn_model)
summary(nn_model)
print(nn_model)
dim(data)
View(data)
X = as.matrix(data[1:ncol(data)-1])
Y = as.matrix(data[,ncol(data)])
nn_model$num_nodes
nn_R <- nnet::nnet(X,Y,size=nn_model$num_nodes[2], trace=FALSE)
nn_R_pred <- as.numeric(stats::predict(nn_R,X))
pROC::roc(data[,ncol(data)],feed_forward(data,nn_model)$pred,
plot=TRUE, print.auc=TRUE, main="ROC curve - BackPropNN")
pROC::roc(data[,ncol(data)],nn_R_pred,
plot=TRUE, print.auc=TRUE, main="ROC curve - R nnet")
sum(nn_R_pred == feed_forward(data,nn_model)$pred)
nn_R_pred
feed_forward(data,nn_model)
library(BackPropNN)
